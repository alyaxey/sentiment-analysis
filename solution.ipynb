{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_json('Microblog_Trainingdata.json.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>cashtag</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>id</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>twitter</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>WYNN</td>\n",
       "      <td>0.489</td>\n",
       "      <td>709499794910109696</td>\n",
       "      <td>[buy <span class=\"tex2jax_ignore\">$</span>WYNN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>twitter</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>DORM</td>\n",
       "      <td>0.414</td>\n",
       "      <td>719732320404709376</td>\n",
       "      <td>[TOP 5 STOCK PICKS]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>DGAZ</td>\n",
       "      <td>0.331</td>\n",
       "      <td>29835080</td>\n",
       "      <td>[Looks like that support line around 24.90 held, t can close green it could start seeing some pstv momentum,]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>twitter</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>HAR</td>\n",
       "      <td>0.454</td>\n",
       "      <td>708355634857529344</td>\n",
       "      <td>[Sector Stocks Leading Today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>FCX</td>\n",
       "      <td>0.339</td>\n",
       "      <td>5514207</td>\n",
       "      <td>[<span class=\"tex2jax_ignore\">$</span>FCX +3.53%]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>twitter</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>CJES</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>CHK</td>\n",
       "      <td>0.279</td>\n",
       "      <td>5364581</td>\n",
       "      <td>[reserves are in decline]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>twitter</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>MAR</td>\n",
       "      <td>0.336</td>\n",
       "      <td>711907079296933888</td>\n",
       "      <td>[Unusual call buying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>RIMM</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>7442585</td>\n",
       "      <td>[So both call/put buyers are crushed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>twitter</td>\n",
       "      <td><span class=\"tex2jax_ignore\">$</span>PYPL</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>719542623279702016</td>\n",
       "      <td>[Barclays signs on to Apple Pay]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          source cashtag  sentiment score                  id  \\\n",
       "353      twitter   $WYNN            0.489  709499794910109696   \n",
       "1014     twitter   $DORM            0.414  719732320404709376   \n",
       "215   stocktwits   $DGAZ            0.331            29835080   \n",
       "560      twitter    $HAR            0.454  708355634857529344   \n",
       "80    stocktwits    $FCX            0.339             5514207   \n",
       "770      twitter   $CJES           -0.438  709834259687710720   \n",
       "23    stocktwits    $CHK            0.279             5364581   \n",
       "1051     twitter    $MAR            0.336  711907079296933888   \n",
       "1695  stocktwits   $RIMM           -0.126             7442585   \n",
       "1205     twitter   $PYPL           -0.113  719542623279702016   \n",
       "\n",
       "                                                                                                              spans  \n",
       "353                                                                                                     [buy $WYNN]  \n",
       "1014                                                                                            [TOP 5 STOCK PICKS]  \n",
       "215   [Looks like that support line around 24.90 held, t can close green it could start seeing some pstv momentum,]  \n",
       "560                                                                                   [Sector Stocks Leading Today]  \n",
       "80                                                                                                    [$FCX +3.53%]  \n",
       "770                                                                                         [Biggest Market Losers]  \n",
       "23                                                                                        [reserves are in decline]  \n",
       "1051                                                                                          [Unusual call buying]  \n",
       "1695                                                                          [So both call/put buyers are crushed]  \n",
       "1205                                                                               [Barclays signs on to Apple Pay]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# printing dataset sample\n",
    "print(len(df))\n",
    "with pd.option_context('display.width', None, 'max_colwidth', 1000, 'max_rows', None):\n",
    "    display(df.sample(10, random_state=31434))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stocktwits    934\n",
       "twitter       766\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring source field values\n",
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AAPL    117\n",
       "$TSLA     80\n",
       "$FB       77\n",
       "$SPY      60\n",
       "$AMZN     30\n",
       "        ... \n",
       "$GG        1\n",
       "$WNR       1\n",
       "$MMM       1\n",
       "$HA        1\n",
       "$OSTK      1\n",
       "Name: cashtag, Length: 684, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring cashtag field values\n",
    "df.cashtag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1145229411764706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fea8bad2e10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exploring sentiment score distribution\n",
    "# we see two clusters and bigger positive one\n",
    "print(df['sentiment score'].mean())\n",
    "df['sentiment score'].hist(bins=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39047349              13\n",
       "719535506980265984    11\n",
       "719520125582503936    10\n",
       "708355634857529344     9\n",
       "709834259687710720     8\n",
       "                      ..\n",
       "26057981               1\n",
       "719532953642999808     1\n",
       "9942259                1\n",
       "9694450                1\n",
       "12799689               1\n",
       "Name: id, Length: 1229, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring id field values\n",
    "df.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1378\n",
       "2     272\n",
       "3      45\n",
       "4       4\n",
       "5       1\n",
       "Name: spans, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring number of spans\n",
    "df.spans.str.len().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fea891636a0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAR2UlEQVR4nO3df7BcZX3H8fcXUCoiEEgMCJRrbZTBWpBGZNSOtDgaTMdQi4w4I8jYpjNFodaZktZ2sK0/0k5rR0ehjQIGWkCqVWj5IRi11lZ+BMQEDNQUg5Dy4yoWdOw4gt/+cZ6Mh2XP3bt3725unrxfM2f27HOe55zn7u757Nlnz9kbmYkkqS577OwOSJLmn+EuSRUy3CWpQoa7JFXIcJekChnuklShvXZ2BwAWL16cU1NTO7sbkrRLue22276bmUv6LVsQ4T41NcXGjRt3djckaZcSEfd1LXNYRpIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklShBXER065ias01fcu3rV054Z5I0sw8cpekChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQnvt7A7UbmrNNX3Lt61dOeGeSNqdeOQuSRUy3CWpQoa7JFXIcJekCg0M94g4PCK+FBHfjIi7IuKcUn5gRNwYEd8qt4tKeUTERyJia0Rsiohjx/1HSJKeajZH7k8A787Mo4DjgbMi4ihgDbAhM5cBG8p9gJOAZWVaDVww772WJM1oYLhn5oOZeXuZ/wGwBTgUWAWsL9XWAyeX+VXAJdm4CTggIg6Z955LkjoNNeYeEVPAS4GbgaWZ+WBZ9BCwtMwfCtzfavZAKetd1+qI2BgRG6enp4fstiRpJrMO94jYF/gM8PuZ+Xh7WWYmkMNsODPXZebyzFy+ZMmSYZpKkgaYVbhHxDNogv0fM/OfS/HDO4Zbyu0jpXw7cHir+WGlTJI0IbM5WyaAC4Etmfmh1qKrgTPK/BnAVa3y08tZM8cDj7WGbyRJEzCb35Z5JfBWYHNE3FHK/hhYC1wZEW8H7gNOLcuuBV4PbAV+BJw5rz2WJA00MNwz86tAdCw+sU/9BM4asV+SpBF4haokVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqtBsfs9dEza15pq+5dvWrpxwTyTtqjxyl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRXabU+F7DrdEDzlUNKuzyN3SaqQ4S5JFTLcJalCu+2Ye238yQJJbR65S1KFDHdJqpDDMrsxh3KkennkLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkio0MNwj4qKIeCQi7myVvTcitkfEHWV6fWvZH0XE1oi4JyJeN66OS5K6zebI/ZPAij7lf5uZx5TpWoCIOAp4M/Di0ub8iNhzvjorSZqdgeGemV8BHp3l+lYBV2TmjzPz28BW4LgR+idJmoNRxtzfERGbyrDNolJ2KHB/q84DpexpImJ1RGyMiI3T09MjdEOS1Guu4X4B8ALgGOBB4G+GXUFmrsvM5Zm5fMmSJXPshiSpnzmFe2Y+nJlPZuZPgY/zs6GX7cDhraqHlTJJ0gTNKdwj4pDW3d8EdpxJczXw5ojYOyKeDywDbhmti5KkYQ38VciIuBw4AVgcEQ8A5wEnRMQxQALbgN8FyMy7IuJK4JvAE8BZmfnkeLouSeoyMNwz87Q+xRfOUP/9wPtH6ZQkaTReoSpJFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFVo4G/LSDtMrbmmc9m2tSsn2BNJg3jkLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKjQw3CPiooh4JCLubJUdGBE3RsS3yu2iUh4R8ZGI2BoRmyLi2HF2XpLU32yO3D8JrOgpWwNsyMxlwIZyH+AkYFmZVgMXzE83JUnDGBjumfkV4NGe4lXA+jK/Hji5VX5JNm4CDoiIQ+ars5Kk2ZnrmPvSzHywzD8ELC3zhwL3t+o9UMokSRM08heqmZlADtsuIlZHxMaI2Dg9PT1qNyRJLXMN94d3DLeU20dK+Xbg8Fa9w0rZ02TmusxcnpnLlyxZMsduSJL6mWu4Xw2cUebPAK5qlZ9ezpo5HnisNXwjSZqQvQZViIjLgROAxRHxAHAesBa4MiLeDtwHnFqqXwu8HtgK/Ag4cwx9liQNMDDcM/O0jkUn9qmbwFmjdkqSNBqvUJWkChnuklQhw12SKmS4S1KFDHdJqpDhLkkVMtwlqUKGuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFRr4k7/SKKbWXNO5bNvalRPsibR78chdkirkkbsWHI/2pdF55C5JFTLcJalChrskVchwl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklQhw12SKmS4S1KFDHdJqlAV/6zDf+4gSU9VRbhLvsFLTzVSuEfENuAHwJPAE5m5PCIOBD4FTAHbgFMz8/ujdVOSNIz5GHP/tcw8JjOXl/trgA2ZuQzYUO5LkiZoHF+orgLWl/n1wMlj2IYkaQajhnsCN0TEbRGxupQtzcwHy/xDwNJ+DSNidURsjIiN09PTI3ZDktQ26heqr8rM7RHxXODGiLi7vTAzMyKyX8PMXAesA1i+fHnfOpKkuRnpyD0zt5fbR4DPAscBD0fEIQDl9pFROylJGs6cwz0inh0Rz9kxD7wWuBO4GjijVDsDuGrUTkqShjPKsMxS4LMRsWM9l2Xm9RFxK3BlRLwduA84dfRuSpKGMedwz8x7gaP7lH8POHGUTkmT0HXhkxc9qQb+towkVchwl6QKGe6SVCHDXZIq5K9CSkPwS1jtKjxyl6QKGe6SVCHDXZIqZLhLUoUMd0mqkOEuSRUy3CWpQoa7JFXIcJekCnmFqjRmXtWqncEjd0mqkOEuSRUy3CWpQoa7JFXIcJekChnuklShBXcqZNdpY+CpY9p9ePqkRuWRuyRVyHCXpAoZ7pJUIcNdkipkuEtShQx3SaqQ4S5JFVpw57lLGp7Xh6iXR+6SVCGP3KXdlEf7dfPIXZIqZLhLUoUMd0mq0NjG3CNiBfBhYE/gE5m5dlzbkjQZcxmnn882fhcwe2M5co+IPYGPAScBRwGnRcRR49iWJOnpxnXkfhywNTPvBYiIK4BVwDfHtD1Ju7m5HO1Pos2kPu30isycVcVhRMQpwIrM/O1y/63AyzPzHa06q4HV5e6LgHs6VrcY+O6QXRi2zSS2YZu5tVmo/bLNwu3X7tTmiMxc0rdFZs77BJxCM86+4/5bgY/OcV0bx91mEtuwjc9NbW0War9s00zjOltmO3B46/5hpUySNAHjCvdbgWUR8fyIeCbwZuDqMW1LktRjLF+oZuYTEfEO4PM0p0JelJl3zXF16ybQZhLbsM3c2izUftlm4fbLNozpC1VJ0s7lFaqSVCHDXZIqZLhLUoUW1O+5R8SRNFeyHlqKtgNXZ+aWMWznUODmzPxhq3xFZl7f0eY4IDPz1vJTCiuAuzPz2iG2e0lmnj5E/VfRXO17Z2be0FHn5cCWzHw8Ip4FrAGOpbka+AOZ+VifNmcDn83M+2fZjx1nPP1PZn4hIt4CvALYAqzLzJ90tPsF4I00p8U+CfwXcFlmPj6b7Uq1iYjnZuYjk9jWgjlyj4hzgSuAAG4pUwCXR8SaOa7zzD5lZwNXAe8E7oyIVa3FH+hYz3nAR4ALIuKDwEeBZwNrIuI9HW2u7pn+BXjjjvsdbW5pzf9O2c5zgPNmeAwuAn5U5j8M7A/8ZSm7uKPNXwA3R8S/R8TvRUT/K9x+5mJgJXBORFwKvAm4GXgZ8ImOv+Vs4O+Anyv19qYJ+Zsi4oQB26tWRDx3Qts5aBLbmW8RsX9ErI2IuyPi0Yj4XkRsKWUHzGF913WU7xcRH4yIS8vBSnvZ+R1tDo6ICyLiYxFxUES8NyI2R8SVEXFIn/oH9kwHAbdExKKIOLBjGyta8/tHxIURsSkiLouIpUP98cNe9TSuieao7hl9yp8JfGuO6/xOn7LNwL5lfgrYCJxT7n+9Yz2baU7p3Ad4HNivlD8L2NTR5nbgH4ATgFeX2wfL/Ks72ny9NX8rsKTMPxvY3NFmS3ubPcvu6NoOzRv7a4ELgWngeuAM4Dl96m8qt3sBDwN7lvsxw9+/uVVvH+DLZf7nux7nsnx/YC1wN/Ao8D2aTwhrgQOGfP6v6yjfD/ggcCnwlp5l53e0ORi4gOYH8Q4C3lv+xiuBQzraHNgzHQRsAxYBB3a0WdHzWFwIbAIuA5Z2tFkLLC7zy4F7ga3AfTO81m4H/gR4wRCP53LgS+V1fThwI/BYea2+tE/9fYE/B+4q9aaBm4C3zbCNzwPnAgf3PPbnAjd0tDm2Y/oV4MGONp8pj9vJNNfgfAbYu99+1GpzPc1B4ZrynJxbHod3Alf1qf9T4Ns900/K7b1dz0tr/hPA+4AjgHcBnxvq9T9M5XFOZWc+ok/5EcA9M7Tb1DFtBn7cp/5dfV6A1wMfYoYw7Ddf7ne12aM8ITcCx5Syvk9oq803aHb8g+i53Lh3u63yfwLOLPMXA8vL/AuBWwe9gMr9ZwBvAC4HpvvUv5PmTXYR8ANKMNEclW/p2Mbm1s6yqP330AwzzcvOvVB37NJmIjs3rTd+mvB9Wes10Pey9dKHvwa+Q/Mp+V3A8wa8Pm+h+aXX04D7gVNK+YnA1/rUvwp4G80V6n8A/CmwDFhPM2TYbxsz7et9l9EM+X2x/O290/91tLmj5/57gP+g2fe6XgPtHPjOTOsrZe8ur5uXtB/3AY/x7TP0sW/WdK5rmMrjnGjGsLcC19GcsL+uPDBbaR3N9Gn3MHBM2QHa0xTNGHFv/S9SwrZVthdwCfBkxzZuBvYp83u0yvfveiG06hxGE8Af7X1B9Km7jeao69vl9pBSvm/XE1v68Engv0s/f1La/htw9KAXaZ9l+/Qpe1dZ533A2cAG4OM0AX5ex3rOoQnBj9O8ce94A1oCfGWG7Q+1cy/UHbuUT2Tnpvlks1eZv6lnWdcnvvZ2fhU4H3ioPG6r5/AYPO01BXyj5/6t5XYPmu+r+m3jBuAPaX1KAZbSvJl+oaPNncCyjmX3z/CY7dFT9jaaTxn3dbT5Rmv+fbN8nHfs/x+iGWIddID3AM0b4bvLPhetZX0/JXeua5jK457Kk3488FtlOp7y0X6GNhcCr+pYdlnHg31wR/1XdpTv3VG+uL3jDujnSjqOVmbRdh/g+QPq7AccTXPE2vfje6vuC+fQh+dRjuyAA2h+HO64AW1eXOodOcR2htq5F/KO3Xq9jXXnpvn0cAPw6zTDRR+mGf77M+DSjjZPexOjGXpcAVzc0eZrNEN5b6J5oz+5lL+aPp8QgP/csW/SfDL8fGtZ11H4IprvjO4Gvk8zNLellHUNZZ0CvKhj2ckd5X8FvKZP+Qo6hoFphpj27VP+i8CnBzyvb6AZknpoQL3zeqYdQ7MHA5cM2n+esq5hKjs5jXvq2bkf7dm5F/Wpv+B37FJvrDs3zXc6n6L5PmUzcC3NT2rv1VH/ijk8N0fTDJtdBxxZ3kT+l+ZN8RV96v8yzVDO94GvUg4qaD69nT3Ddo4EXtP7eDPzJ/gjaYaH5qPNSfO1nXZ9mu/ofmkcf0vf9Qz7BDs57ayJMrQzrvrjbtOzcy+ovk26TVd9mmG/e4DP0QxTrmot6xoym0ubd467zaT61fkYD/skOjntrIkB31mMWt82k2vTVZ+5n8224NpMql9d04K6iEmKiE1di2jG3keqb5vJtZnLNmi+C/khQGZuK9dEfDoijijtdqU2k+pXX4a7FpqlwOtoxmnbguYLulHr22ZybeayjYcj4pjMvAMgM38YEb9Bc7HeS3axNpPqV1+Guxaaf6X5WHpH74KI+PI81LfN5NrMZRunA0+0CzLzCeD0iPj7XazNpPrVl7/nLkkVWjC/LSNJmj+GuyRVyHCXpAoZ7pJUIcNdkir0/96Ra2f6q1DEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# exploring word counts - rather short texts\n",
    "df.spans.str.join(' ').str.split().str.len().value_counts(sort=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                                  20\n",
       "Stochastic Overbought                                             13\n",
       "Unusual Call Activity                                             12\n",
       "Nice day rally!                                                   11\n",
       "downgrade                                                         10\n",
       "                                                                  ..\n",
       "Took Small posistion on $TSLA                                      1\n",
       "wouldn't be surprised to see a green close                         1\n",
       "cash & cheap financing is fueling a wave of acquisitions           1\n",
       "bumping its head on declining daily 8ma as overhead resistance     1\n",
       "staying strong                                                     1\n",
       "Name: spans, Length: 1252, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exploring frequent phrases, notice some empty and finincial slang\n",
    "df.spans.str.join(', ').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part of installation - download needed nltk corpora\n",
    "import nltk\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stocks', 80),\n",
       " ('long', 73),\n",
       " ('buy', 70),\n",
       " ('short', 65),\n",
       " ('today', 51),\n",
       " ('good', 46),\n",
       " ('nice', 44),\n",
       " ('new', 44),\n",
       " ('looking', 43),\n",
       " ('still', 43),\n",
       " ('stock', 41),\n",
       " ('market', 37),\n",
       " ('day', 36),\n",
       " ('bullish', 35),\n",
       " ('top', 34),\n",
       " ('call', 32),\n",
       " ('like', 31),\n",
       " ('week', 28),\n",
       " ('looks', 27),\n",
       " ('trade', 26)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out most common words, notice lot's of financial jargon\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "Counter((w\n",
    "         for w in ' '.join(df.spans.str.join(' ')).lower().split()\n",
    "         if w not in stop_words)\n",
    "       ).most_common()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found main paper about this dataset when googling tweets:  \n",
    "https://www.aclweb.org/anthology/S17-2089.pdf\n",
    "\n",
    "The table with top papers, some with code. Score is not directly comparable because we use cross-validation and they test on a separate dataset. Anyway it's good to stick with their metric. Ideas of Jiang, Ghosal and Cabanski were used in this work.\n",
    "\n",
    "|rank|score|author|paper|code|\n",
    "|-|-|-|-|-|\n",
    "|1|0.778|Jiang|https://www.aclweb.org/anthology/S17-2152.pdf ||\n",
    "|2|0.751|Ghosal|https://www.aclweb.org/anthology/S17-2154.pdf ||\n",
    "|3|0.735|Deborah| https://www.aclweb.org/anthology/S17-2139.pdf ||\n",
    "|4|0.730|Cabanski| https://www.aclweb.org/anthology/S17-2141.pdf | https://github.com/tocab/SemEval2017Task5\n",
    "|5|0.726|Kumar| https://www.aclweb.org/anthology/S17-2153.pdf ||\n",
    "|6|0.723|Kar| http://sudiptakar.info/wp-content/uploads/2018/02/semeval2017task5.pdf | https://github.com/cryptexcode/SemEval-2017-Task-5 |\n",
    "|7|0.720|Nasim| https://www.aclweb.org/anthology/S17-2140.pdf ||\n",
    "|8|0.707|Seyeditabari|https://www.aclweb.org/anthology/S17-2146.pdf ||\n",
    "|9|0.693|Saleiro| https://arxiv.org/pdf/1704.05091.pdf | https://github.com/saleiro/SemEval2017-Task5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded splits\n"
     ]
    }
   ],
   "source": [
    "# saving cross-validation splits for reproducible result\n",
    "# we split by id because unlikely we will see the same tweets in test dataset\n",
    "# this actually makes a couple percent difference with random split and some papers missed it\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import pickle\n",
    "\n",
    "\n",
    "try:\n",
    "    with open('splits.pkl', 'rb') as f:\n",
    "        splits = pickle.load(f)\n",
    "    print('loaded splits')\n",
    "except:\n",
    "    split = GroupKFold(n_splits=5)\n",
    "    splits = list(split.split(df, groups=df.id))\n",
    "    with open('splits.pkl', 'wb') as f:\n",
    "        pickle.dump(splits, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('saved splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we consider both regression to compare to papers and classification as stated in this task\n",
    "# we predict continous y and consider y > 0 as positive and y <=0 as negative\n",
    "# accuracy - as usual\n",
    "# cosine - cosine distance from papers\n",
    "# binary_cosine - similarly to cosine but for classification and calibrated to have max=1 achiveable\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy\n",
    "\n",
    "\n",
    "def main_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true > 0, y_pred > 0),\n",
    "        'cosine': 1 - scipy.spatial.distance.cosine(y_true, y_pred),\n",
    "        'binary_cosine': np.dot(y_true, (y_pred > 0) * 2 - 1) / np.sum(np.abs(y_true)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function that does cross-validation and\n",
    "# calls solve function inside that accepts train and test dataframes\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "def cross_validate(solve):\n",
    "    metrics = []\n",
    "    for train_part, test_part in tqdm(splits, desc='fold'):\n",
    "        test_df = df.loc[test_part, [c for c in df.columns if c != 'sentiment score']]\n",
    "        preds = solve(df.loc[train_part], test_df)\n",
    "        metrics.append(main_metrics(df.loc[test_part, 'sentiment score'], preds))\n",
    "    metrics = pd.DataFrame({c: [m[c] for m in metrics] for c in metrics[0]})\n",
    "    metrics = metrics.apply(lambda x: pd.Series([x.mean(), x.std()])).T\n",
    "    metrics.columns = ['mean', 'std']\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be579201266f4de28e3fc2bb7a7104f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.497647  0.007323\n",
      "cosine        -0.000963  0.041210\n",
      "binary_cosine -0.007022  0.025829\n"
     ]
    }
   ],
   "source": [
    "# baseline solution with random numbers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def solve_random(train_df, test_df):\n",
    "    rng = np.random.RandomState(35345)\n",
    "    return rng.uniform(-1, 1, len(test_df))\n",
    "\n",
    "\n",
    "cross_validate(solve_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a4f335879d4f9686d5144192157e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.642353  0.028026\n",
      "cosine         0.288786  0.042107\n",
      "binary_cosine  0.314406  0.045549\n"
     ]
    }
   ],
   "source": [
    "# baseline solution with always positive class\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def solve_ones(train_df, test_df):\n",
    "    return np.ones(len(test_df))\n",
    "\n",
    "\n",
    "cross_validate(solve_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for downloading from google drive\n",
    "import gdown\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def gdownload(url, dest):\n",
    "    if Path(dest).exists():\n",
    "        print(f'file \"{dest}\" already exists, skipping download')\n",
    "    else:\n",
    "        print(f'downloading file \"{dest}\"')\n",
    "        gdown.download(url, dest, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file \"word2vec_twitter_model.bin\" already exists, skipping download\n"
     ]
    }
   ],
   "source": [
    "# word2vec model referenced in top 2 solution https://www.aclweb.org/anthology/S17-2154.pdf\n",
    "# trained on 400M tweets\n",
    "# downloading 4.5 GB\n",
    "\n",
    "gdownload('https://drive.google.com/uc?id=10B7cvx3xN7Ef_FxwIO8sigd1J1Ibe6Lu',\n",
    "          'word2vec_twitter_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading to python\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "word2vec_twitter = KeyedVectors.load_word2vec_format(\n",
    "    'word2vec_twitter_model.bin',\n",
    "    binary=True,\n",
    "    unicode_errors='ignore',\n",
    "    limit=1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f75a02fc44402090df8ad2637ce5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 68/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.781176  0.024922\n",
      "cosine         0.622414  0.026397\n",
      "binary_cosine  0.640280  0.042294\n"
     ]
    }
   ],
   "source": [
    "# idea from top 2 solution https://www.aclweb.org/anthology/S17-2154.pdf\n",
    "# multilayer perceptron model (MLP) on average word2vec vector\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# get vector for word if exists\n",
    "def word_vec_unk(word2vec, w, unk=None):\n",
    "    try:\n",
    "        return word2vec.word_vec(w)\n",
    "    except KeyError:\n",
    "        return unk\n",
    "\n",
    "\n",
    "# get average vector for each text\n",
    "def df_to_mean_w2v_no_unk(word2vec, df):\n",
    "    result = []\n",
    "    for t in df.spans:\n",
    "        t = word_tokenize(', '.join(t).lower())\n",
    "        t = [word_vec_unk(word2vec, w, None) for w in t]\n",
    "        t = [w for w in t if w is not None]\n",
    "        if len(t) == 0:\n",
    "            t = [np.ones(word2vec.vectors.shape[1])]\n",
    "        t = np.mean(t, axis=0)\n",
    "        result.append(t)\n",
    "    result = np.vstack(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# apply MLP\n",
    "def solve_w2v_mean_mlp(train_df, test_df):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(200, input_dim=400, activation='relu'))\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dropout(rate=0.15))\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    # model doesn't get trained with cosine_proximity as loss by some reason\n",
    "    optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['cosine_proximity'])\n",
    "\n",
    "    model.fit(x=df_to_mean_w2v_no_unk(word2vec_twitter, train_df),\n",
    "              y=train_df['sentiment score'].to_numpy(),\n",
    "              batch_size=64,\n",
    "              epochs=100)\n",
    "\n",
    "    y_pred = model.predict(df_to_mean_w2v_no_unk(word2vec_twitter, test_df))[:, 0]\n",
    "    keras.backend.clear_session()\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "cross_validate(solve_w2v_mean_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725c48c2b0464ba08361836306eb8c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.780588  0.024835\n",
      "cosine         0.614873  0.014540\n",
      "binary_cosine  0.627713  0.045429\n"
     ]
    }
   ],
   "source": [
    "# similar to previous, SVR (support vector regressor) approach on average word2vec vector\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "def solve_w2v_mean_svr(train_df, test_df):\n",
    "    model = SVR(C=1, gamma='scale')\n",
    "    model.fit(df_to_mean_w2v_no_unk(word2vec_twitter, train_df),\n",
    "              train_df['sentiment score'].to_numpy())\n",
    "\n",
    "    y_pred = model.predict(df_to_mean_w2v_no_unk(word2vec_twitter, test_df))\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "cross_validate(solve_w2v_mean_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2047468bf514b1d84f02a26e9b8824d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.805294  0.024905\n",
      "cosine         0.676012  0.028783\n",
      "binary_cosine  0.681180  0.042269\n"
     ]
    }
   ],
   "source": [
    "# bidirectional LSTM with word2vec vectors sequence\n",
    "# idea from https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py\n",
    "# and https://www.aclweb.org/anthology/S17-2154.pdf\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from itertools import cycle, repeat\n",
    "from keras.models import Sequential\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from tensorflow.contrib.opt import AdamWOptimizer\n",
    "\n",
    "\n",
    "# form vector sequences for texts\n",
    "def df_to_emb_seq(word2vec, df):\n",
    "    result = []\n",
    "    for t in df.spans:\n",
    "        t = word_tokenize(', '.join(t).lower())\n",
    "        t = [word_vec_unk(word2vec, w, None) for w in t]\n",
    "        t = [w for w in t if w is not None]\n",
    "        t = t or [np.ones(word2vec.vectors.shape[1], np.float32)]\n",
    "        result.append(np.stack(t))\n",
    "    return result\n",
    "\n",
    "\n",
    "# helper forms minibatches with same length\n",
    "def group_by_len(xs, ys, batch_size):\n",
    "    groups = {}\n",
    "    for i, (x, y) in enumerate(zip(xs, ys if ys is not None else repeat(None))):\n",
    "        groups.setdefault(len(x), []).append((i, x, y))\n",
    "    data = []\n",
    "    inds = []\n",
    "    for g in groups.values():\n",
    "        for b in range(0, len(g), batch_size):\n",
    "            ind, x, y = [np.stack([a[i] for a in g[b: b + batch_size]]) for i in range(3)]\n",
    "            data.append((x, y))\n",
    "            inds.append(ind)\n",
    "    return data, inds\n",
    "\n",
    "\n",
    "# helper puts predictions for minibatches back to original array\n",
    "def preds_to_array(preds, inds):\n",
    "    result = np.zeros_like(preds)\n",
    "    result[np.concatenate(inds)] = preds\n",
    "    return result\n",
    "\n",
    "\n",
    "# apply bidirectional LSTM\n",
    "def solve_w2v_lstm(train_df, test_df):\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(64,\n",
    "                                 dropout=0.4,\n",
    "                                 recurrent_dropout=0.4,\n",
    "                                 return_sequences=True),\n",
    "                            input_shape=(None, word2vec_twitter.vectors.shape[1])))\n",
    "    model.add(Bidirectional(LSTM(64,\n",
    "                                 dropout=0.4,\n",
    "                                 recurrent_dropout=0.4)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    optimizer = 'adam' \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['cosine_proximity'])\n",
    "\n",
    "    data, indexes = group_by_len(df_to_emb_seq(word2vec_twitter, train_df),\n",
    "                                 train_df['sentiment score'], batch_size=32)\n",
    "    model.fit_generator(cycle(data), steps_per_epoch=len(data), epochs=50)\n",
    "\n",
    "    data, indexes = group_by_len(df_to_emb_seq(word2vec_twitter, test_df),\n",
    "                                 None, batch_size=32)\n",
    "    preds = model.predict_generator(iter(data), steps=len(data))\n",
    "    y_pred = preds_to_array(preds[:, 0], indexes)\n",
    "    keras.backend.clear_session()\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "cross_validate(solve_w2v_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272978df6acb4ac2bedff3989ba9df43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.785882  0.023658\n",
      "cosine         0.634629  0.030884\n",
      "binary_cosine  0.658503  0.037476\n"
     ]
    }
   ],
   "source": [
    "# idea from https://www.aclweb.org/anthology/S17-2154.pdf\n",
    "# character ngrams (1-4)\n",
    "# word ngrams (1-4)\n",
    "# gradient boosted regressor on top\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from xgboost import XGBRegressor\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "\n",
    "\n",
    "# join spans for all texts\n",
    "def get_df_text(df):\n",
    "    return df.spans.str.join(', ')\n",
    "\n",
    "\n",
    "# apply pipeline of word n-grams, character n-grams and XGBRegressor\n",
    "def solve_ngrams_xgb(train_df, test_df):\n",
    "    model = Pipeline([\n",
    "        ('get_text', FunctionTransformer(get_df_text, validate=False)),\n",
    "        ('tfidf', FeatureUnion([\n",
    "            ('word', CountVectorizer(ngram_range=(1, 4), min_df=3, binary=True)),\n",
    "            ('char', CountVectorizer(ngram_range=(1, 4), analyzer='char', min_df=3, max_df=0.1, binary=True))])),\n",
    "        ('xgbr', XGBRegressor(objective='reg:squarederror')),\n",
    "    ])\n",
    "\n",
    "    model.fit(train_df, train_df['sentiment score'])\n",
    "    return model.predict(test_df)\n",
    "\n",
    "cross_validate(solve_ngrams_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all lexicons from https://www.aclweb.org/anthology/S17-2141.pdf\n",
    "# this cell - SentiWordNet (Baccianella et al., 2010)\n",
    "# https://github.com/aesuli/SentiWordNet/blob/master/data/SentiWordNet_3.0.0.txt\n",
    "# create dictionary with mean score for stem of each word\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "\n",
    "\n",
    "stem = EnglishStemmer().stem\n",
    "columns = [\"POS\", \"ID\", \"PosScore\", \"NegScore\", \"SynsetTerms\", \"Gloss\"]\n",
    "senti_wordnet_df = pd.read_table('SentiWordNet_3.0.0.txt', names=columns, skiprows=27).iloc[:-1]\n",
    "senti_wordnet = {}\n",
    "for t in senti_wordnet_df.itertuples():\n",
    "    for w in t.SynsetTerms.split():\n",
    "        w = w.partition('#')[0]\n",
    "        senti_wordnet.setdefault(stem(w), []).append(t.PosScore - t.NegScore)\n",
    "\n",
    "senti_wordnet = {k: [np.mean(v)] for k, v in senti_wordnet.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opinion Lexicon (Hu and Liu, 2004)\n",
    "# https://github.com/woodrad/Twitter-Sentiment-Mining/tree/master/Hu%20and%20Liu%20Sentiment%20Lexicon\n",
    "# create dictionary with mean score for stem of each word\n",
    "\n",
    "stem = EnglishStemmer().stem\n",
    "opinion_lexicon = {}\n",
    "for filename, sentiment in [('positive-words.txt', 1), ('negative-words.txt', -1)]:\n",
    "    with open(filename, 'r', encoding='latin1') as f:\n",
    "        for l in f:\n",
    "            l = l.partition(';')[0].strip()\n",
    "            if l:\n",
    "                opinion_lexicon.setdefault(stem(l), []).append(sentiment)\n",
    "opinion_lexicon = {k: [np.mean(v)] for k, v in opinion_lexicon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxDiff Twitter Sentiment Lexicon (Kiritchenko et al., 2014)\n",
    "# https://www.svkir.com/resources.html#manual_lexicons_BWS\n",
    "# create dictionary with mean score for stem of each word\n",
    "\n",
    "stem = EnglishStemmer().stem\n",
    "maxdiff_df = pd.read_csv(\n",
    "    'SemEval2015-English-Twitter-Lexicon.txt',\n",
    "    sep='\\t',\n",
    "    header=None,\n",
    "    names=[\"score\", \"word\"])\n",
    "\n",
    "maxdiff_lexicon = {}\n",
    "for t in maxdiff_df.itertuples():\n",
    "    w = stem(t.word.strip('#'))\n",
    "    maxdiff_lexicon.setdefault(w, []).append(t.score)\n",
    "\n",
    "maxdiff_lexicon = {k: [np.mean(v)] for k, v in maxdiff_lexicon.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9db9d9d4be74e8a85aa875d7da0f3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                   mean       std\n",
      "accuracy       0.746471  0.031085\n",
      "cosine         0.556840  0.039262\n",
      "binary_cosine  0.567239  0.076657\n"
     ]
    }
   ],
   "source": [
    "# model using various lexicons and XGBRegressor\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# get average score for sentence using lexicon dictionary\n",
    "def vocab_score(t, vocab):\n",
    "    t = [vocab.get(w) for w in t]\n",
    "    t = [w for w in t if w is not None]\n",
    "    if len(t) == 0:\n",
    "        t = [np.zeros_like(next(iter(vocab.values())))]\n",
    "    t = np.mean(t, axis=0)\n",
    "    return t\n",
    "\n",
    "\n",
    "# gather all lexicon features for texts\n",
    "def lexicon_features(df):\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    features = []\n",
    "    zeros = np.zeros_like(next(iter(senti_wordnet.values())))\n",
    "    for t in df.spans:\n",
    "        t = ', '.join(t)\n",
    "\n",
    "        # VADER lexicon (Hutto and Gilbert, 2014)\n",
    "        # https://pypi.org/project/vaderSentiment/\n",
    "        vader_scores = analyser.polarity_scores(t)\n",
    "        vader_scores = [vader_scores[k] for k in ['neg', 'neu', 'pos', 'compound']]\n",
    "\n",
    "        # python library for sentiment analysis\n",
    "        # https://textblob.readthedocs.io/en/dev/\n",
    "        textblob_scores = TextBlob(t).sentiment\n",
    "        textblob_scores = [textblob_scores.polarity, textblob_scores.subjectivity]\n",
    "\n",
    "        t = word_tokenize(t.lower())\n",
    "        t = [stem(w) for w in t]\n",
    "        features.append(np.concatenate([\n",
    "            vocab_score(t, senti_wordnet),\n",
    "            vocab_score(t, opinion_lexicon),\n",
    "            vocab_score(t, maxdiff_lexicon),\n",
    "            vader_scores,\n",
    "            textblob_scores]))\n",
    "    return np.stack(features)\n",
    "\n",
    "\n",
    "# join feature extraction and XGBRegressor\n",
    "def solve_lexicons(train_df, test_df):\n",
    "    model = Pipeline([\n",
    "        ('get_text', FunctionTransformer(lexicon_features, validate=False)),\n",
    "        ('xgbr', XGBRegressor(objective='reg:squarederror')),\n",
    "    ])\n",
    "    model.fit(train_df, train_df['sentiment score'])\n",
    "    return model.predict(test_df)\n",
    "\n",
    "\n",
    "cross_validate(solve_lexicons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5fc8ea0d814425b5742aee333aaead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='fold', max=5, style=ProgressStyle(description_width='initial'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5598d0df00f46cc8bc29c30b427bdb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='models', max=5, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 1s 12ms/step - loss: 0.0366 - cosine_proximity: 0.8104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c186660c57740dcb25b85763a8e3ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='models', max=5, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 12ms/step - loss: 0.0332 - cosine_proximity: 0.8398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48359801ff3b447abc19ee6cbf16d001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='models', max=5, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 12ms/step - loss: 0.0299 - cosine_proximity: 0.8514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad1bba5b96842fabbe5976058d00762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='models', max=5, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 12ms/step - loss: 0.0357 - cosine_proximity: 0.8104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7091c5b507a4007866cf2a1371b2794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='models', max=5, style=ProgressStyle(description_width='initia…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 1s 12ms/step - loss: 0.0347 - cosine_proximity: 0.8080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/home/ubuntu/anaconda/envs/sentiment_analysis/lib/python3.6/site-packages/xgboost/core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                   mean       std\n",
      "accuracy       0.832353  0.013638\n",
      "cosine         0.715388  0.021528\n",
      "binary_cosine  0.741198  0.019587\n"
     ]
    }
   ],
   "source": [
    "# Ensemble model - blending\n",
    "# All 5 models are trained on 95% of train data and then predict for 5%\n",
    "# Linear regression is trained on top on these 5% of predictions\n",
    "# Then linear regression and all models outputs are used to predict on test dataset\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def solve_ensemble_linreg(train_df, test_df):\n",
    "    models = [solve_w2v_mean_mlp, solve_w2v_mean_svr, solve_w2v_lstm, solve_ngrams_xgb, solve_lexicons]\n",
    "    train_df = train_df.sample(frac=1, random_state=34324)\n",
    "    train_part = int(len(train_df) * 0.95)\n",
    "    for_pred = pd.concat([train_df[test_df.columns], test_df])\n",
    "    preds = [m(train_df.iloc[:train_part], for_pred) for m in tqdm(models, desc='models')]\n",
    "    preds = np.stack(preds).T\n",
    "    top_model = LinearRegression()\n",
    "    top_model.fit(preds[train_part:len(train_df)], train_df['sentiment score'].iloc[train_part:len(train_df)])\n",
    "    preds = top_model.predict(preds[len(train_df):])\n",
    "    return preds\n",
    "\n",
    "\n",
    "cross_validate(solve_ensemble_linreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\\*Note that results cannot be directly compared to paper because we use cross-validation instead of a separate test dataset. And so our results should be higher.\n",
    "\n",
    "|rank|model|accuracy|cosine|binary_cosine|\n",
    "|-|-|-|-|-|\n",
    "|1|Jiang|  |0.778||\n",
    "|...|...|...|...|...|\n",
    "|7|Nasim|  |0.720||\n",
    "|**8**\\*|**ensemble**|**0.832**|**0.715**|**0.741**|\n",
    "||w2v_lstm|0.805|0.676|0.681|\n",
    "||ngrams_xgb|0.786|0.635|0.659|\n",
    "||mean_mlp|0.781|0.622|0.640|\n",
    "||mean_svr|0.781|0.615|0.628|\n",
    "||lexicons|0.746|0.557|0.567|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    " - FastText vectors give +1% to metric, but they occupy 15 GB memory, so they are not used in the final version. See <https://github.com/FredericGodin/TwitterEmbeddings>. We can also experiment with other embeddings as it makes sense for such small datasets.\n",
    " - More lexicons could be added. There are more examples in top-1 paper.\n",
    " - More ensembling could be done.\n",
    " - Handcrafted features could be added like amount of all-caps words or hashtags in text.\n",
    " - Cashtags and source fields could be used though they didn't show a big difference. Perhaps it's related to the way dataset is annotated.\n",
    " - More modern neural network architectures could be used like BERT.\n",
    " - Transfer learning from top sentiment analysis neural network architectures could be used.\n",
    " - Inverviewing a financial expert may be useful to obtain non-trivial features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
